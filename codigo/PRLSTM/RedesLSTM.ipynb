{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-12 03:00:00</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.25</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-12 04:00:00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.15</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-12 05:00:00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-12 06:00:00</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-12 07:00:00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  open  high   low  close  value\n",
       "0  2020-08-12 03:00:00  3.10  3.35  3.10   3.25     75\n",
       "1  2020-08-12 04:00:00  3.25  3.25  3.15   3.15     75\n",
       "2  2020-08-12 05:00:00  3.15  3.30  3.15   3.30     75\n",
       "3  2020-08-12 06:00:00  3.30  3.30  3.15   3.30     75\n",
       "4  2020-08-12 07:00:00  3.25  3.25  3.20   3.25     75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('SolAtasIMC_tratado.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36400 entries, 0 to 36399\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    36400 non-null  object \n",
      " 1   open    36400 non-null  float64\n",
      " 2   high    36400 non-null  float64\n",
      " 3   low     36400 non-null  float64\n",
      " 4   close   36400 non-null  float64\n",
      " 5   value   36400 non-null  int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-12 03:00:00</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.25</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-12 04:00:00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.15</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-12 05:00:00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-12 06:00:00</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-12 07:00:00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25476</th>\n",
       "      <td>2023-07-11 21:00:00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.90</td>\n",
       "      <td>22.00</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25477</th>\n",
       "      <td>2023-07-11 22:00:00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.90</td>\n",
       "      <td>22.00</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25478</th>\n",
       "      <td>2023-07-11 23:00:00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.75</td>\n",
       "      <td>21.95</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25479</th>\n",
       "      <td>2023-07-12 00:00:00</td>\n",
       "      <td>21.95</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.90</td>\n",
       "      <td>22.05</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25480</th>\n",
       "      <td>2023-07-12 01:00:00</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.10</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25481 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date   open   high    low  close  value\n",
       "0      2020-08-12 03:00:00   3.10   3.35   3.10   3.25     75\n",
       "1      2020-08-12 04:00:00   3.25   3.25   3.15   3.15     75\n",
       "2      2020-08-12 05:00:00   3.15   3.30   3.15   3.30     75\n",
       "3      2020-08-12 06:00:00   3.30   3.30   3.15   3.30     75\n",
       "4      2020-08-12 07:00:00   3.25   3.25   3.20   3.25     75\n",
       "...                    ...    ...    ...    ...    ...    ...\n",
       "25476  2023-07-11 21:00:00  22.00  22.05  21.90  22.00     57\n",
       "25477  2023-07-11 22:00:00  22.00  22.10  21.90  22.00     57\n",
       "25478  2023-07-11 23:00:00  22.00  22.05  21.75  21.95     57\n",
       "25479  2023-07-12 00:00:00  21.95  22.10  21.90  22.05     64\n",
       "25480  2023-07-12 01:00:00  22.05  22.15  22.00  22.10     64\n",
       "\n",
       "[25481 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.copy().loc[0:int(tamanio*0.7)]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25481</th>\n",
       "      <td>2023-07-12 02:00:00</td>\n",
       "      <td>22.10</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.15</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25482</th>\n",
       "      <td>2023-07-12 03:00:00</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.10</td>\n",
       "      <td>22.10</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25483</th>\n",
       "      <td>2023-07-12 04:00:00</td>\n",
       "      <td>22.10</td>\n",
       "      <td>22.10</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25484</th>\n",
       "      <td>2023-07-12 05:00:00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>21.90</td>\n",
       "      <td>21.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25485</th>\n",
       "      <td>2023-07-12 06:00:00</td>\n",
       "      <td>21.95</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.90</td>\n",
       "      <td>22.00</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32756</th>\n",
       "      <td>2024-05-10 05:00:00</td>\n",
       "      <td>153.65</td>\n",
       "      <td>154.35</td>\n",
       "      <td>152.85</td>\n",
       "      <td>153.95</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32757</th>\n",
       "      <td>2024-05-10 06:00:00</td>\n",
       "      <td>153.95</td>\n",
       "      <td>154.70</td>\n",
       "      <td>153.45</td>\n",
       "      <td>153.75</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32758</th>\n",
       "      <td>2024-05-10 07:00:00</td>\n",
       "      <td>153.75</td>\n",
       "      <td>154.10</td>\n",
       "      <td>152.30</td>\n",
       "      <td>153.30</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32759</th>\n",
       "      <td>2024-05-10 08:00:00</td>\n",
       "      <td>153.30</td>\n",
       "      <td>155.10</td>\n",
       "      <td>153.15</td>\n",
       "      <td>154.95</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32760</th>\n",
       "      <td>2024-05-10 09:00:00</td>\n",
       "      <td>154.95</td>\n",
       "      <td>155.75</td>\n",
       "      <td>154.25</td>\n",
       "      <td>154.35</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date    open    high     low   close  value\n",
       "25481  2023-07-12 02:00:00   22.10   22.30   22.05   22.15     64\n",
       "25482  2023-07-12 03:00:00   22.15   22.25   22.10   22.10     64\n",
       "25483  2023-07-12 04:00:00   22.10   22.10   22.00   22.00     64\n",
       "25484  2023-07-12 05:00:00   22.00   22.00   21.90   21.95     64\n",
       "25485  2023-07-12 06:00:00   21.95   22.05   21.90   22.00     64\n",
       "...                    ...     ...     ...     ...     ...    ...\n",
       "32756  2024-05-10 05:00:00  153.65  154.35  152.85  153.95     66\n",
       "32757  2024-05-10 06:00:00  153.95  154.70  153.45  153.75     66\n",
       "32758  2024-05-10 07:00:00  153.75  154.10  152.30  153.30     66\n",
       "32759  2024-05-10 08:00:00  153.30  155.10  153.15  154.95     66\n",
       "32760  2024-05-10 09:00:00  154.95  155.75  154.25  154.35     66\n",
       "\n",
       "[7280 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vali = df.copy().loc[int(tamanio*0.7 + 1):int(tamanio*0.9)]\n",
    "df_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32761</th>\n",
       "      <td>2024-05-10 10:00:00</td>\n",
       "      <td>154.35</td>\n",
       "      <td>154.5</td>\n",
       "      <td>153.45</td>\n",
       "      <td>154.10</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32762</th>\n",
       "      <td>2024-05-10 11:00:00</td>\n",
       "      <td>154.10</td>\n",
       "      <td>154.8</td>\n",
       "      <td>153.25</td>\n",
       "      <td>154.15</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32763</th>\n",
       "      <td>2024-05-10 12:00:00</td>\n",
       "      <td>154.15</td>\n",
       "      <td>154.3</td>\n",
       "      <td>153.25</td>\n",
       "      <td>154.15</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32764</th>\n",
       "      <td>2024-05-10 13:00:00</td>\n",
       "      <td>154.15</td>\n",
       "      <td>155.2</td>\n",
       "      <td>153.00</td>\n",
       "      <td>155.05</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32765</th>\n",
       "      <td>2024-05-10 14:00:00</td>\n",
       "      <td>155.05</td>\n",
       "      <td>155.4</td>\n",
       "      <td>153.10</td>\n",
       "      <td>153.30</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36395</th>\n",
       "      <td>2024-10-08 20:00:00</td>\n",
       "      <td>143.35</td>\n",
       "      <td>143.9</td>\n",
       "      <td>142.35</td>\n",
       "      <td>142.95</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36396</th>\n",
       "      <td>2024-10-08 21:00:00</td>\n",
       "      <td>142.95</td>\n",
       "      <td>144.1</td>\n",
       "      <td>142.25</td>\n",
       "      <td>143.75</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36397</th>\n",
       "      <td>2024-10-08 22:00:00</td>\n",
       "      <td>143.75</td>\n",
       "      <td>144.5</td>\n",
       "      <td>143.35</td>\n",
       "      <td>144.50</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36398</th>\n",
       "      <td>2024-10-08 23:00:00</td>\n",
       "      <td>144.50</td>\n",
       "      <td>144.7</td>\n",
       "      <td>144.05</td>\n",
       "      <td>144.25</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36399</th>\n",
       "      <td>2024-10-09 00:00:00</td>\n",
       "      <td>144.25</td>\n",
       "      <td>144.3</td>\n",
       "      <td>143.55</td>\n",
       "      <td>143.80</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3639 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date    open   high     low   close  value\n",
       "32761  2024-05-10 10:00:00  154.35  154.5  153.45  154.10     66\n",
       "32762  2024-05-10 11:00:00  154.10  154.8  153.25  154.15     66\n",
       "32763  2024-05-10 12:00:00  154.15  154.3  153.25  154.15     66\n",
       "32764  2024-05-10 13:00:00  154.15  155.2  153.00  155.05     66\n",
       "32765  2024-05-10 14:00:00  155.05  155.4  153.10  153.30     66\n",
       "...                    ...     ...    ...     ...     ...    ...\n",
       "36395  2024-10-08 20:00:00  143.35  143.9  142.35  142.95     49\n",
       "36396  2024-10-08 21:00:00  142.95  144.1  142.25  143.75     49\n",
       "36397  2024-10-08 22:00:00  143.75  144.5  143.35  144.50     49\n",
       "36398  2024-10-08 23:00:00  144.50  144.7  144.05  144.25     49\n",
       "36399  2024-10-09 00:00:00  144.25  144.3  143.55  143.80     49\n",
       "\n",
       "[3639 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.copy().loc[int(tamanio*0.9 + 1):tamanio]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valitest = pd.concat([df_vali, df_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numero de horas que se utilizan en la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numhorasconst = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i+n_steps])\n",
    "        y.append(data[i+n_steps, 3])  \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_datosLSTM(df, numhoras):\n",
    "    return create_sequences(df[['open', 'high', 'low', 'close', 'value']].values, numhoras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalRedLSTM(ytest, y_pred):\n",
    "    y_pred = y_pred.flatten()\n",
    "    suma = 0\n",
    "    n = len(y_pred)\n",
    "    for i in range(0,n):\n",
    "        suma = abs(y_pred[i] - ytest[i])/ytest[i] +  suma\n",
    "    error_medio = suma/n\n",
    "    emp = error_medio*100 # error medio en porcentaje\n",
    "    return emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opti_redes_LSTM(epoch_ini, epoch_fin, batch_array, X_trainLSTM, y_trainLSTM, X_valiLSTM, y_valiLSTM, X_testLSTM, y_testLSTM, numhoras):\n",
    "    epoch_best = 0\n",
    "    bacth_best = 0\n",
    "    best_model = None\n",
    "    for e in range(epoch_ini, epoch_fin + 1):\n",
    "        for b in batch_array:\n",
    "            best = 100\n",
    "            for i in range(0, 25):\n",
    "                with tf.device('/CPU:0'):\n",
    "                    modelLSTM = Sequential()\n",
    "                    modelLSTM.add(LSTM(64, activation='relu', input_shape=(numhoras, 5)))\n",
    "                    modelLSTM.add(Dense(1))\n",
    "                    modelLSTM.compile(optimizer='adam', loss='mape')\n",
    "                    historyLSTM = modelLSTM.fit(X_trainLSTM, y_trainLSTM, epochs=e, batch_size=b, validation_data=(X_valiLSTM, y_valiLSTM), shuffle=False)\n",
    "                    y_pred = modelLSTM.predict(X_testLSTM)\n",
    "                    valor = evalRedLSTM(y_testLSTM, y_pred)\n",
    "                    print(\"epoch: \"+str(e)+\", batch_size: \"+str(b)+\", value: \"+str(valor))\n",
    "                    if valor < best:\n",
    "                        best = valor\n",
    "                        epoch_best = e\n",
    "                        bacth_best = b\n",
    "                        best_model = modelLSTM\n",
    "                        if best < 0.75:\n",
    "                            cadena_guardado = \"ModelosLSTMOptiMoreDataIMC/mi_modelo_LSTMOpti_e\"+str(e)+\"_b\"+str(b)+\"_v\"+str(round(valor, 3))\n",
    "                            best_model.save(cadena_guardado+\".keras\")\n",
    "    return epoch_best, bacth_best, valor, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_ARRAY = [4, 8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 96, 128, 192, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opti_rLSTM_h(inih, finh, epoch_ini, epoch_fin, batch_array):\n",
    "    best = 100\n",
    "    epoch_best = 0\n",
    "    bacth_best = 0\n",
    "    h_best = 0\n",
    "    best_model = None\n",
    "    for i in range(inih, finh+1):\n",
    "        Xtrain, ytrain = preparar_datosLSTM(df_train, i)\n",
    "        Xvali, yvali = preparar_datosLSTM(df_vali, i)\n",
    "        Xtest, ytest = preparar_datosLSTM(df_test, i)\n",
    "        valores = opti_redes_LSTM(epoch_ini, epoch_fin, batch_array, Xtrain, ytrain, Xvali, yvali, Xtest, ytest, i)\n",
    "        if valores[2] < best:\n",
    "            best = valores[2]\n",
    "            epoch_best = valores[0]\n",
    "            bacth_best = valores[1]\n",
    "            h_best = i\n",
    "            best_model = valores[3]\n",
    "            cadena_guardado = \"ModelosLSTMOptiMoreDataIMCBest/mi_modelo_LSTM_Opti_e\"+str(epoch_best)+\"_b\"+str(bacth_best)+\"_h\"+str(i)+\"_v\"+str(round(best, 3))\n",
    "            best_model.save(cadena_guardado+\".keras\")\n",
    "    return best, epoch_best, bacth_best, h_best, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 4.1207 - val_loss: 7.9412\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.1628 - val_loss: 6.4863\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 1.8445 - val_loss: 4.2426\n",
      "114/114 [==============================] - 0s 770us/step\n",
      "epoch: 3, batch_size: 4, value: 6.599586735492495\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 4.8310 - val_loss: 17.0595\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.4367 - val_loss: 7.6668\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 1.5942 - val_loss: 3.3262\n",
      "114/114 [==============================] - 0s 779us/step\n",
      "epoch: 3, batch_size: 4, value: 5.551235425095374\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 3.2407 - val_loss: 10.6266\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.1690 - val_loss: 4.3904\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 1.9152 - val_loss: 1.9582\n",
      "114/114 [==============================] - 0s 796us/step\n",
      "epoch: 3, batch_size: 4, value: 3.1239801769784603\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 5.8511 - val_loss: 8.1012\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.2525 - val_loss: 12.9969\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 1.9987 - val_loss: 32.8211\n",
      "114/114 [==============================] - 0s 814us/step\n",
      "epoch: 3, batch_size: 4, value: 63.80537261429373\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 4.6678 - val_loss: 17.3258\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 2.6448 - val_loss: 5.6586\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.3720 - val_loss: 4.6973\n",
      "114/114 [==============================] - 0s 796us/step\n",
      "epoch: 3, batch_size: 4, value: 8.954031046273716\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 5.9694 - val_loss: 15.6501\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 3.8791 - val_loss: 11.9683\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.9117 - val_loss: 6.9608\n",
      "114/114 [==============================] - 0s 805us/step\n",
      "epoch: 3, batch_size: 4, value: 12.40564086324178\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 4.0217 - val_loss: 14.6524\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.2621 - val_loss: 6.7567\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.5060 - val_loss: 3.2313\n",
      "114/114 [==============================] - 0s 805us/step\n",
      "epoch: 3, batch_size: 4, value: 5.463313984752139\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 4.6798 - val_loss: 11.5455\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.6693 - val_loss: 6.8008\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.1385 - val_loss: 5.0723\n",
      "114/114 [==============================] - 0s 788us/step\n",
      "epoch: 3, batch_size: 4, value: 8.586646459434625\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 3.6516 - val_loss: 13.4136\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.1901 - val_loss: 8.1748\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 9s 1ms/step - loss: 2.2632 - val_loss: 6.8780\n",
      "114/114 [==============================] - 0s 796us/step\n",
      "epoch: 3, batch_size: 4, value: 11.292690097351752\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 12s 2ms/step - loss: 5.6305 - val_loss: 20.3032\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 3.7750 - val_loss: 14.6001\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 2.0364 - val_loss: 6.8326\n",
      "114/114 [==============================] - 0s 912us/step\n",
      "epoch: 3, batch_size: 4, value: 11.571164484604877\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 2.9630 - val_loss: 11.3700\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.4800 - val_loss: 8.9904\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.3310 - val_loss: 7.3105\n",
      "114/114 [==============================] - 0s 850us/step\n",
      "epoch: 3, batch_size: 4, value: 11.043913003388948\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.1187 - val_loss: 9.5914\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 2.4948 - val_loss: 5.4603\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 1.8271 - val_loss: 3.4518\n",
      "114/114 [==============================] - 0s 823us/step\n",
      "epoch: 3, batch_size: 4, value: 5.806991736675208\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 4.8207 - val_loss: 5.0191\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.0654 - val_loss: 6.1895\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.7585 - val_loss: 5.5061\n",
      "114/114 [==============================] - 0s 823us/step\n",
      "epoch: 3, batch_size: 4, value: 4.1802685651232405\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 5.0721 - val_loss: 8.2906\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.0125 - val_loss: 9.0659\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 2.0932 - val_loss: 4.2896\n",
      "114/114 [==============================] - 0s 823us/step\n",
      "epoch: 3, batch_size: 4, value: 7.963931490474688\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.5183 - val_loss: 10.6084\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 2.7059 - val_loss: 4.4124\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 1ms/step - loss: 2.4895 - val_loss: 2.0023\n",
      "114/114 [==============================] - 0s 805us/step\n",
      "epoch: 3, batch_size: 4, value: 3.0473711524655793\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 4.1336 - val_loss: 12.4602\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 12s 2ms/step - loss: 2.4957 - val_loss: 6.3842\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 1.8816 - val_loss: 3.6757\n",
      "114/114 [==============================] - 0s 1ms/step\n",
      "epoch: 3, batch_size: 4, value: 6.22839364448243\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 12s 2ms/step - loss: 3.2519 - val_loss: 9.2241\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 2.7201 - val_loss: 4.0084\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.5666 - val_loss: 4.1570\n",
      "114/114 [==============================] - 0s 920us/step\n",
      "epoch: 3, batch_size: 4, value: 7.056811748172649\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 3.7191 - val_loss: 15.7827\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.5887 - val_loss: 6.8379\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.1297 - val_loss: 3.7395\n",
      "114/114 [==============================] - 0s 867us/step\n",
      "epoch: 3, batch_size: 4, value: 6.338396360717646\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.6771 - val_loss: 10.2056\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.8884 - val_loss: 5.8158\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.4794 - val_loss: 3.4003\n",
      "114/114 [==============================] - 0s 823us/step\n",
      "epoch: 3, batch_size: 4, value: 5.86678171198433\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.7981 - val_loss: 8.7790\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.8356 - val_loss: 5.0782\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.2016 - val_loss: 5.5025\n",
      "114/114 [==============================] - 0s 823us/step\n",
      "epoch: 3, batch_size: 4, value: 9.68729201029037\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.2342 - val_loss: 8.2495\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.4551 - val_loss: 4.6882\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 1.9115 - val_loss: 3.6739\n",
      "114/114 [==============================] - 0s 814us/step\n",
      "epoch: 3, batch_size: 4, value: 6.0259773397959835\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 12s 2ms/step - loss: 4.1845 - val_loss: 17.4259\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.8264 - val_loss: 10.4455\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.3315 - val_loss: 3.8117\n",
      "114/114 [==============================] - 0s 920us/step\n",
      "epoch: 3, batch_size: 4, value: 7.122316112246084\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 3.7936 - val_loss: 13.5686\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.6903 - val_loss: 5.8811\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 1.9886 - val_loss: 3.8100\n",
      "114/114 [==============================] - 0s 876us/step\n",
      "epoch: 3, batch_size: 4, value: 6.342893273742344\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 11s 2ms/step - loss: 4.5550 - val_loss: 13.7125\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.0266 - val_loss: 6.8092\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.2969 - val_loss: 3.8705\n",
      "114/114 [==============================] - 0s 850us/step\n",
      "epoch: 3, batch_size: 4, value: 8.092386266926594\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 3.0807 - val_loss: 7.8377\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.5845 - val_loss: 6.8059\n",
      "Epoch 3/3\n",
      "6369/6369 [==============================] - 10s 2ms/step - loss: 2.1770 - val_loss: 9.4160\n",
      "114/114 [==============================] - 0s 814us/step\n",
      "epoch: 3, batch_size: 4, value: 13.422072013276132\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 4.1416 - val_loss: 6.7405\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 2.8194 - val_loss: 1.6975\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 2.3891 - val_loss: 3.8788\n",
      "114/114 [==============================] - 0s 832us/step\n",
      "epoch: 3, batch_size: 8, value: 6.672171198184487\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 3.7675 - val_loss: 7.0803\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 3.1135 - val_loss: 11.3270\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 3.0775 - val_loss: 8.8480\n",
      "114/114 [==============================] - 0s 814us/step\n",
      "epoch: 3, batch_size: 8, value: 9.007068067376117\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 3.1391 - val_loss: 4.7200\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 2.8284 - val_loss: 4.2368\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 2.5424 - val_loss: 2.8236\n",
      "114/114 [==============================] - 0s 973us/step\n",
      "epoch: 3, batch_size: 8, value: 4.782804079086249\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 3.9921 - val_loss: 11.7424\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 2.9617 - val_loss: 9.0279\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 2.6665 - val_loss: 4.9032\n",
      "114/114 [==============================] - 0s 938us/step\n",
      "epoch: 3, batch_size: 8, value: 7.920717173369242\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 4.0205 - val_loss: 11.1157\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 3.2437 - val_loss: 2.5511\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 2.8192 - val_loss: 9.1078\n",
      "114/114 [==============================] - 0s 876us/step\n",
      "epoch: 3, batch_size: 8, value: 13.950839623659327\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 6.4928 - val_loss: 10.4758\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 5.0029 - val_loss: 7.0361\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 3.2980 - val_loss: 1.9347\n",
      "114/114 [==============================] - 0s 841us/step\n",
      "epoch: 3, batch_size: 8, value: 5.71054898953375\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 4.1199 - val_loss: 8.3463\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 3.3500 - val_loss: 7.0348\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 2.5253 - val_loss: 7.2982\n",
      "114/114 [==============================] - 0s 841us/step\n",
      "epoch: 3, batch_size: 8, value: 13.288423488403472\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 3.5293 - val_loss: 3.2286\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 2.9804 - val_loss: 1.8870\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 5s 2ms/step - loss: 2.6877 - val_loss: 1.6917\n",
      "114/114 [==============================] - 0s 841us/step\n",
      "epoch: 3, batch_size: 8, value: 1.3531702036361122\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 6.5354 - val_loss: 16.4850\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 6s 2ms/step - loss: 4.0043 - val_loss: 12.1516\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 3.0419 - val_loss: 8.5739\n",
      "114/114 [==============================] - 0s 1ms/step\n",
      "epoch: 3, batch_size: 8, value: 14.533955958600057\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 3.5416 - val_loss: 3.4782\n",
      "Epoch 2/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 2.7613 - val_loss: 7.1900\n",
      "Epoch 3/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 2.8190 - val_loss: 10.3045\n",
      "114/114 [==============================] - 0s 1ms/step\n",
      "epoch: 3, batch_size: 8, value: 17.075564851810224\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/3\n",
      "3185/3185 [==============================] - 7s 2ms/step - loss: 4.6602 - val_loss: 20.7619\n",
      "Epoch 2/3\n",
      " 507/3185 [===>..........................] - ETA: 4s - loss: 6.9882"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mopti_rLSTM_h\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_ARRAY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m, in \u001b[0;36mopti_rLSTM_h\u001b[1;34m(inih, finh, epoch_ini, epoch_fin, batch_array)\u001b[0m\n\u001b[0;32m      9\u001b[0m Xvali, yvali \u001b[38;5;241m=\u001b[39m preparar_datosLSTM(df_vali, i)\n\u001b[0;32m     10\u001b[0m Xtest, ytest \u001b[38;5;241m=\u001b[39m preparar_datosLSTM(df_test, i)\n\u001b[1;32m---> 11\u001b[0m valores \u001b[38;5;241m=\u001b[39m \u001b[43mopti_redes_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_fin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXvali\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myvali\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valores[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m best:\n\u001b[0;32m     13\u001b[0m     best \u001b[38;5;241m=\u001b[39m valores[\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mopti_redes_LSTM\u001b[1;34m(epoch_ini, epoch_fin, batch_array, X_trainLSTM, y_trainLSTM, X_valiLSTM, y_valiLSTM, X_testLSTM, y_testLSTM, numhoras)\u001b[0m\n\u001b[0;32m     12\u001b[0m modelLSTM\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     13\u001b[0m modelLSTM\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m historyLSTM \u001b[38;5;241m=\u001b[39m \u001b[43mmodelLSTM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trainLSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trainLSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valiLSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valiLSTM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m modelLSTM\u001b[38;5;241m.\u001b[39mpredict(X_testLSTM)\n\u001b[0;32m     16\u001b[0m valor \u001b[38;5;241m=\u001b[39m evalRedLSTM(y_testLSTM, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\raulg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = opti_rLSTM_h(7, 16, 3, 20, BATCH_ARRAY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
